{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics with Python!\n",
    "\n",
    "In this class, we're going to learn how to perform basic statistical operations with Python. We're going to be using a couple of different libraries, namely numpy, scipy and the Python standard library. Afterwards, we can go thorough an introduction for the Pandas library if we have time. \n",
    "\n",
    "\n",
    "At the end, we'll perform these operations on a real life dataset to get up to speed with these operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start with importing the numpy library as np\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "incomes = np.random.normal(50000,10000,10000)\n",
    "np.mean(incomes) #Very simple to compute mean, by simply calling np.mean(<<your_variable>>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(incomes,50)\n",
    "plt.title('Normal Distribution of Incomes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(incomes) #Medians are also very easy to compute with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "incomes = np.append(incomes, [1000000000]) #Append an outlier into your distrubution. What happens to the mean and mode?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(incomes) # Always think about what outliers could do to your data, and change around the mean and the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(incomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mode\n",
    "\n",
    "Mode is the most common value within a dataset. This is pretty easy to understand, and can be useful for smaller (read: less spaced out) datasets.\n",
    "\n",
    "Age data is a good one to be using with mode for example, becaues you can get the most common age of a population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = np.random.randint(20, high=95, size=2000)\n",
    "ages.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "stats.mode(ages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Variation & Standard Deviation\n",
    "\n",
    "Probably something you've heard before, but let's talk about how we can use them, and what they really are. \n",
    "A loose definition: they measure the 'spread' of a data distribution.\n",
    "\n",
    "Variance and Standard Distributions are about the spread of the distribution, or the dataset. \n",
    "\n",
    "![Twitter Updates Histogram](img/twitter_updates.png)\n",
    "\n",
    "How do you calculate variance?\n",
    "![Variance Calc](img/variance.jpg)\n",
    "\n",
    "And Standard Deviaton? That's just the square root of sigma squared (== sigma). \n",
    "\n",
    "How many sigmas from the mean is it? \n",
    "\n",
    "\n",
    "### Population vs Sample Differences\n",
    "\n",
    "Technical mathematical difference, but important nonetheless. If you are studying wages for a population, the population would include ALL people who are earning wages. If you take a subset out of the population, it is a sample. \n",
    "\n",
    "How does that change anything? Well, when calculating variance, when you divide your squared differences by N (the size of the dataset), you only divide by N if you have the entire population. If you are working with a sample, you divide by (N-1). \n",
    "\n",
    "Here are the mathematical formulae for population and sample variance:\n",
    "![Variance Formula](img/variance_formula.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitializing our histogram to give you perspective on variance\n",
    "incomes = np.random.normal(20000,10000,10000)\n",
    "plt.hist(incomes,50)\n",
    "plt.title('Normal Distribution of Incomes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomes.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomes.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Density Functions\n",
    "\n",
    "Probability density functions just give you the chance or probability of a value falling within a certain range. These are only for continuous data. For discrete data, they're called probability mass functions.\n",
    "\n",
    "### Uniform Distribution\n",
    "\n",
    "Starting with something trivial but important, the uniform distribution is very simple to visualise. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting a uniform distribution\n",
    "values = np.random.uniform(-5,5,10000)\n",
    "plt.hist(values,50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Probabilities\n",
    "\n",
    "Centered around the mean (0), with sigma standard deviations. \n",
    "\n",
    "![Normal](img/normal.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "x = np.arange(-3, 3, 0.001)\n",
    "plt.plot(x, norm.pdf(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 5.0\n",
    "sigma = 2.0\n",
    "values = np.random.normal(mu, sigma, 10000)\n",
    "plt.hist(values, 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential PDF/ Power Law\n",
    "\n",
    "Think of scientific decay or perhaps things dropping off dramatically quickly. \n",
    "\n",
    "This is what a exponential PDF looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import expon\n",
    "\n",
    "x = np.arange(0.001, 10, 0.001)\n",
    "plt.plot(x, expon.pdf(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binomial Probability Mass Function\n",
    "\n",
    "Mass functions are for discrete data remember! The binomial distribution takes two parameters, n and p, where it measures the number of successes in n experiments, each of which yields a success probability p.\n",
    "\n",
    "e.g. Coin toss method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom\n",
    "n, p = 10, 0.5\n",
    "x = np.arange(0, 10, 0.001)\n",
    "plt.plot(x, binom.pmf(x, n, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poission Probability Mass Distribution\n",
    "\n",
    "Looks like normal, but has a different application. If you have information about the average number of things that happen during a time period, you can use possion distribution to calculate the odds of getting another number on a given future date. \n",
    "\n",
    "e.g. Website hits per day for a year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import poisson\n",
    "mu = 500\n",
    "x = np.arange(400, 600, 0.5)\n",
    "plt.plot(x, poisson.pmf(x, mu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Percentiles & Moments\n",
    "\n",
    "Percentiles have a simple idea, and you hear about them a lot. Simple to compute in Python. If you scores for a test were in xth percentile, then your scores were higher than x% of the people who took the test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = np.random.normal(0, 0.5, 10000)\n",
    "\n",
    "plt.hist(vals, 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(vals,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(vals,99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moments on the other hand are harder mathematically, but really really simple conceptually. \n",
    "\n",
    "![Moments](img/moments.jpg)\n",
    "\n",
    "First moment is the mean.\n",
    "Second moment is the variance.\n",
    "Third moment is skew: how lopsided the distribution is. e.g. longer tails\n",
    "Fourth moment is the measure of kurtosis: how thick the tail is and how high the peak is.\n",
    "\n",
    "That's it! Easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = np.random.normal(0, 0.5, 10000)\n",
    "\n",
    "plt.hist(vals, 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as sp\n",
    "sp.skew(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.kurtosis(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance & Correlation\n",
    "\n",
    "Let's say you have two different attributes to something, and you want to see if they are related or not. That's what these things measure!\n",
    "\n",
    "Covariance measures how two variables vary in tandem from their means. Before we go to computation, let's talk about what this means. \n",
    "\n",
    "A small covariance (close to 0) means that there is not much correlation between two variables.\n",
    "\n",
    "But a large covariance (or a large negative one for inverse relationships) means that there is correlation. \n",
    "\n",
    "But what does large mean?\n",
    "\n",
    "That's where correlation comes in use. Correlation is calculated by dividing the covariance by the standard deviation of both variables and that will normalize everything. For correlation, -1 is perfect inverse correlation, 0 is no correlation, and 1 is perfect correlation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's say we work for Amazon, and they are interested in finding a correlation between page load times and how much a customer spends.\n",
    "\n",
    "numpy offers covariance methods, but we'll do it the hard way to see the mechanics of it. Basically we treat each variable as a vector of deviations from the mean, and compute the \"dot product\" of both vectors. Geometrically this can be thought of as the angle between the two vectors in a high-dimensional space, but you can just think of it as a measure of similarity between the two variables.\n",
    "\n",
    "First, let's just make page load times and amount spent totally random and independent of each other; a very small covariance will result as there is no real correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "\n",
    "def dev_from_mean(x):\n",
    "    xmean = mean(x)\n",
    "    return [xi - xmean for xi in x]\n",
    "\n",
    "def covariance(x, y):\n",
    "    n = len(x)\n",
    "    return dot(dev_from_mean(x), dev_from_mean(y)) / (n-1)\n",
    "\n",
    "pageSpeeds = np.random.normal(3.0, 1.0, 1000)\n",
    "purchaseAmount = np.random.normal(50.0, 10.0, 1000)\n",
    "\n",
    "scatter(pageSpeeds, purchaseAmount)\n",
    "\n",
    "covariance (pageSpeeds, purchaseAmount)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we fed it just random data, but now let's try to make a relationship between pageSpeeds and purchaseAmount. I've done that for you below. Check what the correlation gives you now! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchaseAmount = np.random.normal(50.0, 10.0, 1000) / pageSpeeds\n",
    "\n",
    "scatter(pageSpeeds, purchaseAmount)\n",
    "\n",
    "covariance (pageSpeeds, purchaseAmount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What does that covariance mean though? Let's convert it to correlation (manually!)\n",
    "def correlation(x, y):\n",
    "    stddevx = x.std()\n",
    "    stddevy = y.std()\n",
    "    return covariance(x,y) / stddevx / stddevy  #Assuming we're not dividing by 0 here\n",
    "\n",
    "correlation(pageSpeeds, purchaseAmount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Of course, there is a far easier way with numpy.\n",
    "\n",
    "np.corrcoef(pageSpeeds, purchaseAmount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The array shows you the correlations between both the variables and also itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a perfectly linear relationship to showcase that the 'idea' of correlations and covariances work\n",
    "purchaseAmount = 100 + pageSpeeds * 4\n",
    "\n",
    "scatter(pageSpeeds, purchaseAmount)\n",
    "\n",
    "correlation (pageSpeeds, purchaseAmount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remember: Correlation != Causation! \n",
    "\n",
    "\n",
    "# P.S. - The easy way to calculate covariance is using np.cov()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
